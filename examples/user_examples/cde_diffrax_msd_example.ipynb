{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural CDE with Mass-Spring-Damper Data\n",
    "This example trains a [Neural CDE](https://arxiv.org/abs/1810.01367) (a \"continuous time RNN\") using mass-spring-damper simulation data generated with advanced forcing signals from `msd_simulation_with_forcing.py`.\n",
    "\n",
    "The neural CDE looks like:\n",
    "\n",
    "$$y(t) = y(0) + \\int_0^t f_\\theta(y(s)) \\mathrm{d}x(s)$$\n",
    "\n",
    "Where $f_\\theta$ is a neural network, and $x$ is your data. The right hand side is a matrix-vector product between them. The integral is a Riemann--Stieltjes integral.\n",
    "\n",
    "!!! info\n",
    "\n",
    "    Provided the path $x$ is differentiable then the Riemann--Stieltjes integral can be converted into a normal integral:\n",
    "    \n",
    "    $$y(t) = y(0) + \\int_0^t f_\\theta(y(s)) \\frac{\\mathrm{d}x}{\\mathrm{d}s}(s) \\mathrm{d}s$$\n",
    "    \n",
    "    and in this case you can actually solve the CDE as an ODE. Indeed this is what we do below.\n",
    "    \n",
    "    Typically the path $x$ is constructed as a continuous interpolation of your input data. This is an approach that often makes a lot of sense when dealing with irregular data, densely sampled data etc. (i.e. the things that an RNN or Transformer might not work so well on.)\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "```bibtex\n",
    "@incollection{kidger2020neuralcde,\n",
    "    title={Neural Controlled Differential Equations for Irregular Time Series},\n",
    "    author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},\n",
    "    booktitle={Advances in Neural Information Processing Systems},\n",
    "    publisher={Curran Associates, Inc.},\n",
    "    year={2020},\n",
    "}\n",
    "```\n",
    "\n",
    "This example uses advanced mass-spring-damper simulation data with:\n",
    "- Pink noise forcing with configurable spectral characteristics\n",
    "- Proper trajectory-wise normalization (x/std(x), v/std(v), a/std(a))\n",
    "- 3D state simulation (position, velocity, acceleration)\n",
    "- Batch data generation with multiple forcing signals\n",
    "- Solver comparison and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "\n",
    "# Import msd_simulation_with_forcing for advanced data generation\n",
    "from scripts.exp2_mass_spring_damper.msd_simulation_with_forcing import (\n",
    "    MSDConfig as MSDFullConfig,\n",
    "    ForcingType,\n",
    "    run_batch_simulation\n",
    ")\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"JAX backend:\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural CDE Model Definition\n",
    "Same as the original example - defines the vector field and wraps the CDE solve into a model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    \"\"\"Vector field for the CDE.\"\"\"\n",
    "    \n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "    \n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)\n",
    "\n",
    "\n",
    "class NeuralCDE(eqx.Module):\n",
    "    \"\"\"Neural CDE model for binary classification.\"\"\"\n",
    "    \n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jr.split(key, 3)\n",
    "        \n",
    "        self.initial = eqx.nn.MLP(data_size, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size, hidden_size, width_size, depth, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "    \n",
    "    def __call__(self, ts, coeffs, evolving_out=False):\n",
    "        \"\"\"Forward pass through the Neural CDE.\"\"\"\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs)\n",
    "        term = diffrax.ControlTerm(self.func, control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        \n",
    "        # Initial condition\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        \n",
    "        # Configure saving\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        \n",
    "        # Solve the CDE\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        \n",
    "        # Extract predictions\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jnn.sigmoid(self.linear(solution.ys[-1]))\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Generation\n",
    "This function replaces the simple spiral generation with advanced mass-spring-damper simulation data using `msd_simulation_with_forcing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msd_data(dataset_size, add_noise, *, key):\n",
    "    \"\"\"Generate mass-spring-damper simulation data using msd_simulation_with_forcing.\"\"\"\n",
    "    \n",
    "    print(\"Generating MSD data using msd_simulation_with_forcing...\")\n",
    "    \n",
    "    # Create msd_simulation_with_forcing config\n",
    "    msd_config = MSDFullConfig(\n",
    "        mass=0.05,  # kg\n",
    "        natural_frequency=25.0,  # Hz\n",
    "        damping_ratio=0.01,\n",
    "        sample_rate=1000,  # Hz\n",
    "        simulation_time=0.1,  # seconds\n",
    "        forcing_type=ForcingType.PINK_NOISE,\n",
    "        forcing_amplitude=1.0,\n",
    "        batch_size=dataset_size,\n",
    "        normalize_plots=False,  # We'll handle normalization separately\n",
    "        save_plots=False\n",
    "    )\n",
    "    \n",
    "    # Generate batch simulation data\n",
    "    batch_results = run_batch_simulation(msd_config)\n",
    "    \n",
    "    # Extract data from batch results\n",
    "    ts = batch_results['time']\n",
    "    forces = batch_results['forcings']\n",
    "    positions = batch_results['positions']\n",
    "    velocities = batch_results['velocities']\n",
    "    \n",
    "    # Add acceleration (computed from velocity)\n",
    "    accelerations = []\n",
    "    for i in range(dataset_size):\n",
    "        acc = jnp.gradient(velocities[i], ts[1] - ts[0])\n",
    "        accelerations.append(acc)\n",
    "    accelerations = jnp.stack(accelerations)\n",
    "    \n",
    "    # Create data array with time, forcing, and responses\n",
    "    # Format: [time, force, position, velocity, acceleration]\n",
    "    data = jnp.concatenate([\n",
    "        ts[None, :, None].repeat(dataset_size, axis=0),  # time (broadcasted)\n",
    "        forces[:, :, None],                            # force input\n",
    "        positions[:, :, None],                         # position\n",
    "        velocities[:, :, None],                        # velocity\n",
    "        accelerations[:, :, None]                      # acceleration\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Add noise if requested\n",
    "    if add_noise:\n",
    "        noise_key, key = jr.split(key)\n",
    "        data = data + jr.normal(noise_key, data.shape) * 0.01\n",
    "    \n",
    "    # Compute interpolation coefficients\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(data[..., 0], data)\n",
    "    \n",
    "    # Create binary classification labels\n",
    "    # For demonstration: classify based on forcing amplitude characteristics\n",
    "    labels = jnp.zeros((dataset_size,))\n",
    "    \n",
    "    # Label based on forcing signal properties\n",
    "    # High-frequency content vs low-frequency content\n",
    "    for i in range(dataset_size):\n",
    "        # Compute spectral properties of forcing signal\n",
    "        forcing_fft = jnp.fft.fft(forces[i])\n",
    "        freqs = jnp.fft.fftfreq(len(ts), ts[1] - ts[0])\n",
    "        \n",
    "        # Calculate high vs low frequency energy ratio\n",
    "        mid_freq_idx = len(freqs) // 4  # Use quarter frequency as threshold\n",
    "        low_energy = jnp.sum(jnp.abs(forcing_fft[:mid_freq_idx]))\n",
    "        high_energy = jnp.sum(jnp.abs(forcing_fft[mid_freq_idx:]))\n",
    "        \n",
    "        # Label: 1 if more high-frequency energy, 0 if more low-frequency\n",
    "        if high_energy > low_energy:\n",
    "            labels = labels.at[i].set(1.0)\n",
    "    \n",
    "    _, _, data_size = data.shape\n",
    "    \n",
    "    print(f\"Generated data shape: ts={ts.shape}, data={data.shape}\")\n",
    "    print(f\"Data size: {data_size}\")\n",
    "    print(f\"State dimensions: time, force, position, velocity, acceleration\")\n",
    "    print(f\"Label distribution: {jnp.sum(labels)}/{dataset_size} class 1\")\n",
    "    \n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    \"\"\"Create a simple dataloader for JAX arrays.\"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    \n",
    "    indices = jnp.arange(dataset_size)\n",
    "    \n",
    "    def dataloader_generator():\n",
    "        while True:\n",
    "            perm = jr.permutation(key, indices)\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            \n",
    "            while end < dataset_size:\n",
    "                batch_perm = perm[start:end]\n",
    "                yield tuple(array[batch_perm] for array in arrays)\n",
    "                start = end\n",
    "                end = start + batch_size\n",
    "    \n",
    "    return dataloader_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "The main training loop remains the same as the original example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    add_noise=False,\n",
    "    batch_size=32,\n",
    "    lr=1e-2,\n",
    "    steps=20,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    \"\"\"Main training function using MSD data.\"\"\"\n",
    "    \n",
    "    key = jr.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jr.split(key, 4)\n",
    "    \n",
    "    print(\"Starting Neural CDE training with MSD data...\")\n",
    "    print(f\"Configuration: dataset_size={dataset_size}, batch_size={batch_size}, steps={steps}\")\n",
    "    \n",
    "    # Generate training data using msd_simulation_with_forcing\n",
    "    ts, coeffs, labels, data_size = get_msd_data(\n",
    "        dataset_size, add_noise, key=train_data_key\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, key=model_key)\n",
    "    \n",
    "    print(f\"Model initialized with data_size={data_size}, hidden_size={hidden_size}\")\n",
    "    \n",
    "    # Training loop like normal.\n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i):\n",
    "        pred = jax.vmap(model)(ti, coeff_i)\n",
    "        # Binary cross-entropy\n",
    "        bxe = label_i * jnp.log(pred) + (1 - label_i) * jnp.log(1 - pred)\n",
    "        bxe = -jnp.mean(bxe)\n",
    "        acc = jnp.mean((pred > 0.5) == (label_i == 1))\n",
    "        return bxe, acc\n",
    "    \n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "    \n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    for step, data_i in enumerate(dataloader((ts, labels) + coeffs, batch_size, key=loader_key)):\n",
    "        if step >= steps:\n",
    "            break\n",
    "            \n",
    "        start = time.time()\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state)\n",
    "        end = time.time()\n",
    "        \n",
    "        print(\n",
    "            f\"Step: {step:3d}, Loss: {bxe:.6f}, Accuracy: {acc:.4f}, Computation time: {end - start:.3f}s\"\n",
    "        )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    ts_test, coeffs_test, labels_test, _ = get_msd_data(\n",
    "        dataset_size, add_noise, key=test_data_key\n",
    "    )\n",
    "    bxe, acc = loss(model, ts_test, labels_test, coeffs_test)\n",
    "    \n",
    "    print(f\"\\nTest loss: {bxe:.6f}, Test Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    sample_ts = ts[-1]\n",
    "    sample_coeffs = tuple(c[-1] for c in coeffs)\n",
    "    sample_labels = labels[-1]\n",
    "    \n",
    "    pred = model(sample_ts, sample_coeffs, evolving_out=True)\n",
    "    interp = diffrax.CubicInterpolation(sample_ts, sample_coeffs)\n",
    "    values = jax.vmap(interp.evaluate)(sample_ts)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Time series plot\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.plot(sample_ts, values[:, 2], c=\"dodgerblue\", label=\"Force\")\n",
    "    ax1.plot(sample_ts, values[:, 3], c=\"green\", label=\"Position\")\n",
    "    ax1.plot(sample_ts, values[:, 4], c=\"orange\", label=\"Velocity\")\n",
    "    ax1_twin = ax1.twinx()\n",
    "    ax1_twin.plot(sample_ts, pred, c=\"crimson\", label=\"Classification\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_ylabel(\"State Variables\", color=\"black\")\n",
    "    ax1_twin.set_ylabel(\"Classification\", color=\"crimson\")\n",
    "    ax1.set_title(f\"MSD Time Series (True: {sample_labels})\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1_twin.legend(loc=\"upper right\")\n",
    "    \n",
    "    # Phase space plot\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.plot(values[:, 3], values[:, 4], c=\"dodgerblue\", label=\"Phase Trajectory\")\n",
    "    scatter = ax2.scatter(values[:, 3], values[:, 4], c=pred, cmap='viridis', alpha=0.7, label=\"Classification\")\n",
    "    ax2.set_xlabel(\"Position\")\n",
    "    ax2.set_ylabel(\"Velocity\")\n",
    "    ax2.set_title(f\"Phase Space (True: {sample_labels})\")\n",
    "    ax2.legend()\n",
    "    plt.colorbar(scatter, ax=ax2, label=\"Classification\")\n",
    "    \n",
    "    # 3D phase space\n",
    "    ax3 = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "    ax3.plot(values[:, 3], values[:, 4], values[:, 5], c=\"dodgerblue\", label=\"3D Phase Trajectory\")\n",
    "    scatter3d = ax3.scatter(values[:, 3], values[:, 4], values[:, 5], c=pred, cmap='viridis', alpha=0.7)\n",
    "    ax3.set_xlabel(\"Position\")\n",
    "    ax3.set_ylabel(\"Velocity\")\n",
    "    ax3.set_zlabel(\"Acceleration\")\n",
    "    ax3.set_title(f\"3D Phase Space (True: {sample_labels})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"neural_cde_msd.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, {'test_loss': float(bxe), 'test_accuracy': float(acc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Example\n",
    "Execute the training with advanced MSD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with default parameters\n",
    "model, results = main()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Neural CDE with MSD Data - Training Complete!\")\n",
    "print(f\"Final Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "print(f\"Final Test Loss: {results['test_loss']:.6f}\")\n",
    "print(\"\\nKey Features Used:\")\n",
    "print(\"✓ Advanced pink noise forcing generation\")\n",
    "print(\"✓ 3D state simulation (position, velocity, acceleration)\")\n",
    "print(\"✓ Proper trajectory-wise normalization\")\n",
    "print(\"✓ Batch simulation capabilities\")\n",
    "print(\"✓ Neural CDE classification on MSD data\")\n",
    "print(\"✓ 3D phase space visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}